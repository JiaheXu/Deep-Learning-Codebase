{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "## 访问参数，用于调试、诊断和可视化。\n",
    "## 参数初始化。\n",
    "## 在不同模型组件间共享参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.random.uniform((2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-0.34566057],\n",
       "       [ 0.26663178]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(8, activation=None),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Dense(1, activation=None),\n",
    "])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## **访问指定block/layer**\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n",
    "\n",
    "## Sequential生成一个layers (List),  layers[2]对应第二个Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(8, 1) dtype=float32, numpy=\n",
      "array([[ 0.04037005],\n",
      "       [ 0.598933  ],\n",
      "       [ 0.22802722],\n",
      "       [-0.27683067],\n",
      "       [-0.05554366],\n",
      "       [-0.70629275],\n",
      "       [-0.42406684],\n",
      "       [-0.18650591]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[2].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "## **访问指定block/层 的参数**\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "bias:  <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n",
      "data:  tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"type: \",type(net.layers[2].weights[1]))\n",
    "print(\"bias: \",net.layers[2].weights[1])\n",
    "print(\"data: \",tf.convert_to_tensor(net.layers[2].weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(4, 8) dtype=float32, numpy=\n",
      "array([[ 0.04382765,  0.696186  , -0.36402014, -0.20503885, -0.41400647,\n",
      "        -0.12040496, -0.41194162, -0.38887328],\n",
      "       [-0.39199367,  0.27840507, -0.6206715 ,  0.132712  , -0.38921165,\n",
      "         0.10012811, -0.485865  , -0.6455044 ],\n",
      "       [ 0.3991484 , -0.70613164,  0.51576585,  0.6392707 , -0.45439595,\n",
      "         0.3729859 ,  0.02352452,  0.22074729],\n",
      "       [ 0.00746876, -0.1892041 , -0.34542274,  0.64455026, -0.22019434,\n",
      "         0.19063592, -0.4160983 ,  0.04846805]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n",
      "[array([[ 0.04382765,  0.696186  , -0.36402014, -0.20503885, -0.41400647,\n",
      "        -0.12040496, -0.41194162, -0.38887328],\n",
      "       [-0.39199367,  0.27840507, -0.6206715 ,  0.132712  , -0.38921165,\n",
      "         0.10012811, -0.485865  , -0.6455044 ],\n",
      "       [ 0.3991484 , -0.70613164,  0.51576585,  0.6392707 , -0.45439595,\n",
      "         0.3729859 ,  0.02352452,  0.22074729],\n",
      "       [ 0.00746876, -0.1892041 , -0.34542274,  0.64455026, -0.22019434,\n",
      "         0.19063592, -0.4160983 ,  0.04846805]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.04037005],\n",
      "       [ 0.598933  ],\n",
      "       [ 0.22802722],\n",
      "       [-0.27683067],\n",
      "       [-0.05554366],\n",
      "       [-0.70629275],\n",
      "       [-0.42406684],\n",
      "       [-0.18650591]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[0].weights)\n",
    "print(net.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04037005],\n",
       "       [ 0.598933  ],\n",
       "       [ 0.22802722],\n",
       "       [-0.27683067],\n",
       "       [-0.05554366],\n",
       "       [-0.70629275],\n",
       "       [-0.42406684],\n",
       "       [-0.18650591]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_weights()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "## **从嵌套块收集参数**\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-0.02606016],\n",
       "       [-0.00657137]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1(name):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\n",
    "        name=name)\n",
    "\n",
    "def block2():\n",
    "    net = tf.keras.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add(block1(name=f'block {i}'))\n",
    "        #net.add(block1() ) must has a name\n",
    "    return net\n",
    "\n",
    "rgnet = tf.keras.Sequential()\n",
    "rgnet.add(block2())\n",
    "rgnet.add(tf.keras.layers.Dense(1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]\n",
    "# 输出整个网络结构\n",
    "## 由于一开始有两个Dense() 所以这里的dense_10应该是dense_8\n",
    "## 结果可以和pytorch版对上 pytorch里bias默认的初始方式不是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    multiple                  304       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  5         \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet.layers[0].layers[1].layers[0].weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "  \n",
    "  \n",
    "# Parameter Initialization !!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "默认情况下，Keras会根据一个范围均匀地初始化权重矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "偏置参数设置为0。\n",
    "TensorFlow在根模块和`keras.initializers`模块中提供了各种初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "# Initialization API\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_11/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.00632504,  0.0105204 , -0.00167855, -0.00059901],\n",
       "        [-0.00705013, -0.01026072, -0.00831791,  0.02173584],\n",
       "        [ 0.01864938,  0.00324481, -0.0014407 ,  0.02041347],\n",
       "        [ 0.01144907,  0.01510851,  0.01114926, -0.01029254]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_11/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        #################################\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "        #################################\n",
    "    tf.keras.layers.Dense(1)])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_13/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_13/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(1),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_16/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "    tf.keras.layers.Dense(\n",
    "        1, kernel_initializer=tf.keras.initializers.Constant(1)),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "# **自定义初始化**\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 54,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "在这里，我们定义了一个`Initializer`的子类，\n",
    "并实现了`__call__`函数。\n",
    "该函数返回给定形状和数据类型的所需张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 57,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_17/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 5.7147903,  7.73114  ,  6.0629005,  5.856309 ],\n",
      "       [-9.138217 ,  0.       ,  5.5772   ,  0.       ],\n",
      "       [-6.6191053, -0.       , -8.561266 ,  8.402901 ],\n",
      "       [-7.3769116, -7.161014 ,  7.3570175, -7.026937 ]], dtype=float32)>, <tf.Variable 'dense_17/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        data=tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor=(tf.abs(data) >= 5)\n",
    "        factor=tf.cast(factor, tf.float32)\n",
    "        return data * factor\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=MyInit()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[0].weights[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 58
   },
   "source": [
    "# 我们始终直接设置参数。\n",
    "# .assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 61,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_18/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[42.        ],\n",
       "       [ 0.69250834],\n",
       "       [ 1.2613361 ],\n",
       "       [ 1.0540156 ]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\n",
    "net.layers[1].weights[0][0, 0].assign(42)\n",
    "net.layers[1].weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 63
   },
   "source": [
    "# 参数共享\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "origin_pos": 66,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.keras的表现有点不同。它会自动删除重复层\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "# 检查参数是否不同\n",
    "print(len(net.layers) == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras.initializers.Initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.initializers.initializers_v2.HeUniform at 0x1b054f8da30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.initializers.Zeros()\n",
    "tf.keras.initializers.Ones()\n",
    "tf.keras.initializers.Constant(value=0)\n",
    "tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "# 按照截尾正态分布生成随机张量的初始化器。\n",
    "# 生成的随机值与 RandomNormal 生成的类似，但是在距离平均值两个标准差之外的随机值将被丢弃并重新生成。\n",
    "# 这是用来生成神经网络权重和滤波器的推荐初始化器。\n",
    "tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "\n",
    "# 初始化器能够根据权值的尺寸调整其规模。\n",
    "tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in',  distribution='normal', seed=None)\n",
    "\n",
    "tf.keras.initializers.Orthogonal(gain=1.0, seed=None)\n",
    "\n",
    "# 它从 [-limit，limit] 中的均匀分布中抽取样本， \n",
    "# 其中 limit 是 sqrt(3 / fan_in)， fan_in 是权值张量中的输入单位的数量。\n",
    "tf.keras.initializers.lecun_uniform(seed=None)\n",
    "\n",
    "# 它从以 0 为中心，标准差为 stddev = sqrt(1 / fan_in) 的截断正态分布中抽取样本，\n",
    "# 其中 fan_in 是权值张量中的输入单位的数量。\n",
    "tf.keras.initializers.lecun_normal(seed=None)\n",
    "\n",
    "# glorot_normal\n",
    "# Glorot 正态分布初始化器，也称为 Xavier 正态分布初始化器。\n",
    "# 它从以 0 为中心，标准差为 stddev = sqrt(2 / (fan_in + fan_out)) 的截断正态分布中抽取样本， \n",
    "# 其中 fan_in 是权值张量中的输入单位的数量， fan_out 是权值张量中的输出单位的数量。\n",
    "tf.keras.initializers.glorot_normal(seed=None)\n",
    "\n",
    "# Glorot 均匀分布初始化器，也称为 Xavier 均匀分布初始化器。\n",
    "# 它从 [-limit，limit] 中的均匀分布中抽取样本， 其中 limit 是 sqrt(6 / (fan_in + fan_out))，\n",
    "# fan_in 是权值张量中的输入单位的数量， fan_out 是权值张量中的输出单位的数量。\n",
    "tf.keras.initializers.glorot_uniform(seed=None)\n",
    "\n",
    "# he_normal\n",
    "# 它从以 0 为中心，标准差为 stddev = sqrt(2 / fan_in) 的截断正态分布中抽取样本， \n",
    "# 其中 fan_in 是权值张量中的输入单位的数量\n",
    "tf.keras.initializers.he_normal(seed=None)\n",
    "\n",
    "# he_uniform\n",
    "# 它从 [-limit，limit] 中的均匀分布中抽取样本， \n",
    "# 其中 limit 是 sqrt(6 / fan_in)， 其中 fan_in 是权值张量中的输入单位的数量。\n",
    "tf.keras.initializers.he_uniform(seed=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
