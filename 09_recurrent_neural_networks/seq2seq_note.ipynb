{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f15f69",
   "metadata": {},
   "source": [
    "# seq2seq总结: \n",
    "## seq2seq: 机器翻译etc A->B， AB都是语言\n",
    "\n",
    "## 所有model都是encoder decoder\n",
    "## encoder输入的是A语言的list\n",
    "## encoder的输出是经过 某RNNmodel 后的 每个词对应的feature和最后一个hidden_state (多层的话取成为output之前的hidden_state)\n",
    "`for X in inputs:`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`Y = torch.mm(H, W_hq) + b_q`  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`outputs.append(Y)`  \n",
    "\n",
    "## decoder的输入是encoder最后的hidden_state和B语言当前的单词\n",
    "## decoder在生成下一个状态的时候要用当前的B语言单词的embedding和一个上下文向量context 拼在一起\n",
    "## 普通版的context就是encoder最后的hidden_state\n",
    "## 普通注意力机制生成一个更与单词embedding相关的context,这个context是以当前hidden_state为query，enc_output为key的插值，当前context是enc_output的线性组合\n",
    "## transformer是在普通注意力机制上的改进\n",
    "\n",
    "## 对于encoder：训练时,用A语言的序列进encoder,得到enc_output和final hidden_state\n",
    "## 对于decoder:   训练时,numstep一般比较长(大于1) predict时numstep就是1，一个单词一个单词地预测\n",
    "## 对于普通的seq2seq 可以直接把所有数据按(numstep,batch,embed_size)传入decoder\n",
    "## 对于有attention的model，要把数据(numstep,batch,embed_size) 按(batch,embed_size)一步一步往decoder传\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969067ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e0030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
